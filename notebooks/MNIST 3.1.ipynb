{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST 3.1 - Convolutional \n",
    "\n",
    "This document serves as my notes on the Deep Learning Without a PhD seminar by Google. \n",
    "\n",
    "* [Video](https://www.youtube.com/watch?v=vq2nnJ4g6N0)\n",
    "* [Slides - 1](https://docs.google.com/presentation/d/1TVixw6ItiZ8igjp6U17tcgoFrLSaHWQmMOwjlgQY9co/pub?slide=id.p)\n",
    "* [Slides - 2](https://docs.google.com/presentation/d/e/2PACX-1vRouwj_3cYsmLrNNI3Uq5gv5-hYp_QFdeoan2GlxKgIZRSejozruAbVV0IMXBoPsINB7Jw92vJo2EAM/pub?slide=id.p)\n",
    "\n",
    "This notebook covers the example of trying to classify MNIST digits with:\n",
    "* a **bigger** convolutional network than in 3.0, \n",
    "* as well as using **bigger** filters, giving us more degrees of freedom\n",
    "* dropout on the fully connected layer\n",
    "\n",
    "**Note:** We dont have dropout on the conv layers, because we are trying to add degrees of freedom there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data as mnist_data\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Download images and labels into mnist.test (10K images+labels) and mnist.train (60K images+labels)\n",
    "mnist = mnist_data.read_data_sets(\"data\", one_hot=True, reshape=False, validation_size=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Model\n",
    "\n",
    "In this notebook, we will code up the following model:\n",
    "\n",
    "![](images/conv-model-2.png)\n",
    "\n",
    "**Note:** The output channels of one layer, matches the input channels of the next layer.\n",
    "\n",
    "For clarity, we can define all our sizes upfront: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchSize = None;       # put None for right now as we dont know yet\n",
    "\n",
    "imgHeight = 28\n",
    "imgWidth = 28\n",
    "numOfColors = 1         # gray scale images\n",
    "\n",
    "numberOfClasses = 10    # 10 classes: 0-9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the size of our convolutional layers. **Note**: This corresponds to **number of different filters** we are going to use on this layer. Visually, this is the number of output/slices we produce above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Size of each layer:\n",
    "sizeLayerOne = 6\n",
    "sizeLayerTwo = 12\n",
    "sizeLayerThree = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the sizes of our fully connected layer, and the output layer as before: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sizeLayerFour = 200\n",
    "sizeLayerFive = numberOfClasses       # the final layer is the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define the stride size, according to the image above. According to [this stackoverflow question](https://stackoverflow.com/questions/34642595/tensorflow-strides-argument): \n",
    "\n",
    "The first 1 is the batch: You don't usually want to skip over examples in your batch, or you shouldn't have included them in the first place. :)\n",
    "\n",
    "The last 1 is the depth of the convolution: You don't usually want to skip inputs, for the same reason.\n",
    "\n",
    "The conv2d operator is more general, so you could create convolutions that slide the window along other dimensions, but that's not a typical use in convnets. The typical use is to use them spatially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stridesL1 = [1,1,1,1]\n",
    "stridesL2 = [1,2,2,1]\n",
    "stridesL3 = [1,2,2,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, lets begin with our inputs and placeholders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_img = tf.placeholder(tf.float32, [batchSize, imgHeight, imgWidth, numOfColors], name=\"X_img\")\n",
    "Y_True = tf.placeholder(tf.float32, [batchSize, 10])\n",
    "learningRate = tf.placeholder(tf.float32)\n",
    "probKeep = tf.placeholder(tf.float32, name=\"Prob_Keep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a global variable holding our dropout prob:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PROBKEEP = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can actually define our layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Layer_1_Conv\"):\n",
    "    W1 = tf.Variable(tf.truncated_normal([6,6,1,sizeLayerOne], stddev=0.1), name=\"Weights\")\n",
    "    \n",
    "    b1 = tf.Variable(tf.ones([sizeLayerOne])/10, name=\"Bias\")\n",
    "    \n",
    "    Y1 = tf.nn.relu(tf.nn.conv2d(X_img, W1, strides=stridesL1, padding='SAME') + b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Layer_2_Conv\"):\n",
    "    W2 = tf.Variable(tf.truncated_normal([5,5,sizeLayerOne,sizeLayerTwo], stddev=0.1), name=\"Weights\")\n",
    "    \n",
    "    b2 = tf.Variable(tf.ones([sizeLayerTwo])/10, name=\"Bias\")\n",
    "    \n",
    "    Y2 = tf.nn.relu(tf.nn.conv2d(Y1, W2, strides=stridesL2, padding='SAME') + b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Layer_3_Conv\"):\n",
    "    W3 = tf.Variable(tf.truncated_normal([4,4,sizeLayerTwo,sizeLayerThree], stddev=0.1), name=\"Weights\")\n",
    "    \n",
    "    b3 = tf.Variable(tf.ones([sizeLayerThree])/10, name=\"Bias\")\n",
    "    \n",
    "    Y3 = tf.nn.relu(tf.nn.conv2d(Y2, W3, strides=stridesL3, padding='SAME') + b3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We need to be careful about sizes - as we can see in the image above, the result of this layer will be a $7 \\times 7 \\times 12$ tensor. We want to flatten this, so we need a fully connected layer with a weight matrix of size: $7 \\times 7 \\times \\texttt{sizeLayerThree}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Layer_4_FC\"):\n",
    "    W4 = tf.Variable(tf.truncated_normal([7*7*sizeLayerThree,sizeLayerFour], stddev=0.1), name=\"Weights\")\n",
    "    \n",
    "    b4 = tf.Variable(tf.ones([sizeLayerFour])/10, name=\"Bias\")\n",
    "    \n",
    "    # flatten the last layers output\n",
    "    Y3_Flat = tf.reshape(Y3, shape=[-1, 7 * 7 * sizeLayerThree], name=\"flatten\")\n",
    "    \n",
    "    Y4 = tf.nn.relu(tf.matmul(Y3_Flat, W4) + b4)\n",
    "    \n",
    "    # Dropout\n",
    "    Y4_Dropout = tf.nn.dropout(Y4, probKeep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Output_Layer\"):\n",
    "    W5 = tf.Variable(tf.truncated_normal([sizeLayerFour, sizeLayerFive], stddev=0.1), name=\"Weights\")\n",
    "    \n",
    "    b5 = tf.Variable(tf.ones([sizeLayerFive])/10, name=\"Bias\")\n",
    "    \n",
    "    Y_logits = tf.matmul(Y4_Dropout, W5) + b5\n",
    "    \n",
    "    Y_Pred = tf.nn.softmax(Y_logits, name=\"Activation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "As before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalized cross entropy loss\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Y_logits, labels=Y_True)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)*100\n",
    "\n",
    "# accuracy of the trained model, between 0 (worst) and 1 (best)\n",
    "correct_prediction = tf.equal(tf.argmax(Y_Pred, 1), tf.argmax(Y_True, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# training step\n",
    "trainStep = tf.train.AdamOptimizer(learningRate).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "numberOfBatches = 10000\n",
    "batchSize = 100\n",
    "\n",
    "trainingAccuracyList = []\n",
    "trainingLossList = []\n",
    "testAccuracyList = []\n",
    "testLossList = []\n",
    "\n",
    "# learning rate decay\n",
    "maxLearningRate = 0.003\n",
    "minLearningRate = 0.0001\n",
    "decaySpeed = 2000.0   # 0.003-0.0001-2000=>0.9826 done in 5000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number:  0 lr:  0.003 Test Loss:  273.419\n",
      "Batch number:  100 lr:  0.0028585653310520707 Test Loss:  16.0462\n",
      "Batch number:  200 lr:  0.0027240285123042826 Test Loss:  9.62034\n",
      "Batch number:  300 lr:  0.0025960531316326675 Test Loss:  8.41056\n",
      "Batch number:  400 lr:  0.0024743191839261473 Test Loss:  6.36464\n",
      "Batch number:  500 lr:  0.002358522270907074 Test Loss:  5.3494\n",
      "Batch number:  600 lr:  0.002248372839976982 Test Loss:  5.20454\n",
      "Batch number:  700 lr:  0.002143595460184269 Test Loss:  4.70025\n",
      "Batch number:  800 lr:  0.002043928133503354 Test Loss:  4.47475\n",
      "Batch number:  900 lr:  0.001949121639703143 Test Loss:  4.7859\n",
      "Batch number:  1000 lr:  0.0018589389131666372 Test Loss:  4.09029\n",
      "Batch number:  1100 lr:  0.0017731544501034114 Test Loss:  3.66008\n",
      "Batch number:  1200 lr:  0.001691553744672677 Test Loss:  4.0337\n",
      "Batch number:  1300 lr:  0.0016139327526069466 Test Loss:  3.13788\n",
      "Batch number:  1400 lr:  0.0015400973809950877 Test Loss:  3.33295\n",
      "Batch number:  1500 lr:  0.0014698630029489428 Test Loss:  3.58433\n",
      "Batch number:  1600 lr:  0.0014030539959399427 Test Loss:  3.4864\n",
      "Batch number:  1700 lr:  0.0013395033026513076 Test Loss:  3.29803\n",
      "Batch number:  1800 lr:  0.0012790520132477375 Test Loss:  3.60876\n",
      "Batch number:  1900 lr:  0.0012215489680180538 Test Loss:  3.31714\n",
      "Batch number:  2000 lr:  0.0011668503793971828 Test Loss:  3.20587\n",
      "Batch number:  2100 lr:  0.0011148194724223506 Test Loss:  3.15618\n",
      "Batch number:  2200 lr:  0.0010653261427244307 Test Loss:  3.09407\n",
      "Batch number:  2300 lr:  0.0010182466311992545 Test Loss:  2.77465\n",
      "Batch number:  2400 lr:  0.0009734632145453863 Test Loss:  2.89684\n",
      "Batch number:  2500 lr:  0.0009308639108945514 Test Loss:  2.95276\n",
      "Batch number:  2600 lr:  0.0008903421997986366 Test Loss:  2.78561\n",
      "Batch number:  2700 lr:  0.0008517967558730855 Test Loss:  2.65453\n",
      "Batch number:  2800 lr:  0.0008151311954306589 Test Loss:  2.90838\n",
      "Batch number:  2900 lr:  0.0007802538354720133 Test Loss:  2.74828\n",
      "Batch number:  3000 lr:  0.0007470774644304465 Test Loss:  2.6024\n",
      "Batch number:  3100 lr:  0.0007155191240975549 Test Loss:  3.25992\n",
      "Batch number:  3200 lr:  0.0006854999021845007 Test Loss:  2.7704\n",
      "Batch number:  3300 lr:  0.000656944735000187 Test Loss:  3.01469\n",
      "Batch number:  3400 lr:  0.0006297822197529307 Test Loss:  3.1657\n",
      "Batch number:  3500 lr:  0.000603944436006291 Test Loss:  2.86872\n",
      "Batch number:  3600 lr:  0.000579366775842601 Test Loss:  2.76821\n",
      "Batch number:  3700 lr:  0.0005559877823095201 Test Loss:  2.61604\n",
      "Batch number:  3800 lr:  0.0005337489957456418 Test Loss:  2.91992\n",
      "Batch number:  3900 lr:  0.0005125948076008895 Test Loss:  2.85253\n",
      "Batch number:  4000 lr:  0.0004924723213861769 Test Loss:  2.77786\n",
      "Batch number:  4100 lr:  0.0004733312204046323 Test Loss:  2.85814\n",
      "Batch number:  4200 lr:  0.00045512364193364755 Test Loss:  2.80741\n",
      "Batch number:  4300 lr:  0.00043780405754314123 Test Loss:  2.77109\n",
      "Batch number:  4400 lr:  0.00042132915925076824 Test Loss:  2.88954\n",
      "Batch number:  4500 lr:  0.00040565775122940656 Test Loss:  3.09337\n",
      "Batch number:  4600 lr:  0.0003907506467961309 Test Loss:  2.86888\n",
      "Batch number:  4700 lr:  0.0003765705704250939 Test Loss:  3.02668\n",
      "Batch number:  4800 lr:  0.0003630820645392963 Test Loss:  2.84449\n",
      "Batch number:  4900 lr:  0.00035025140084817447 Test Loss:  2.8116\n",
      "Batch number:  5000 lr:  0.00033804649600930654 Test Loss:  2.7818\n",
      "Batch number:  5100 lr:  0.0003264368314033442 Test Loss:  2.73019\n",
      "Batch number:  5200 lr:  0.0003153933768215683 Test Loss:  2.91214\n",
      "Batch number:  5300 lr:  0.00030488851787524585 Test Loss:  3.06025\n",
      "Batch number:  5400 lr:  0.0002948959869452743 Test Loss:  2.92549\n",
      "Batch number:  5500 lr:  0.00028539079749945195 Test Loss:  3.05488\n",
      "Batch number:  5600 lr:  0.00027634918161313215 Test Loss:  2.98829\n",
      "Batch number:  5700 lr:  0.0002677485305370315 Test Loss:  3.04394\n",
      "Batch number:  5800 lr:  0.000259567338163581 Test Loss:  3.13875\n",
      "Batch number:  5900 lr:  0.00025178514725045394 Test Loss:  2.86435\n",
      "Batch number:  6000 lr:  0.00024438249826680544 Test Loss:  2.97242\n",
      "Batch number:  6100 lr:  0.00023734088073430873 Test Loss:  2.90129\n",
      "Batch number:  6200 lr:  0.00023064268694131766 Test Loss:  3.03094\n",
      "Batch number:  6300 lr:  0.00022427116791441654 Test Loss:  3.03063\n",
      "Batch number:  6400 lr:  0.00021821039153726202 Test Loss:  2.83583\n",
      "Batch number:  6500 lr:  0.00021244520271199385 Test Loss:  2.87309\n",
      "Batch number:  6600 lr:  0.00020696118546359606 Test Loss:  2.94968\n",
      "Batch number:  6700 lr:  0.0002017446268924506 Test Loss:  3.07399\n",
      "Batch number:  6800 lr:  0.00019678248288494563 Test Loss:  2.95953\n",
      "Batch number:  6900 lr:  0.00019206234549639704 Test Loss:  2.99407\n",
      "Batch number:  7000 lr:  0.00018757241192472366 Test Loss:  2.94429\n",
      "Batch number:  7100 lr:  0.00018330145499729437 Test Loss:  3.06324\n",
      "Batch number:  7200 lr:  0.00017923879509714843 Test Loss:  3.07748\n",
      "Batch number:  7300 lr:  0.0001753742734583905 Test Loss:  3.08989\n",
      "Batch number:  7400 lr:  0.00017169822676398424 Test Loss:  3.00731\n",
      "Batch number:  7500 lr:  0.00016820146298242642 Test Loss:  2.99945\n",
      "Batch number:  7600 lr:  0.00016487523838288026 Test Loss:  3.05909\n",
      "Batch number:  7700 lr:  0.0001617112356712938 Test Loss:  3.0633\n",
      "Batch number:  7800 lr:  0.00015870154319283275 Test Loss:  3.16441\n",
      "Batch number:  7900 lr:  0.00015583863514862209 Test Loss:  3.15453\n",
      "Batch number:  8000 lr:  0.00015311535277732913 Test Loss:  3.14871\n",
      "Batch number:  8100 lr:  0.0001505248864545312 Test Loss:  3.09058\n",
      "Batch number:  8200 lr:  0.00014806075866510764 Test Loss:  3.09702\n",
      "Batch number:  8300 lr:  0.00014571680780607802 Test Loss:  3.12816\n",
      "Batch number:  8400 lr:  0.00014348717277938534 Test Loss:  3.07084\n",
      "Batch number:  8500 lr:  0.00014136627833609785 Test Loss:  3.05151\n",
      "Batch number:  8600 lr:  0.00013934882113538272 Test Loss:  3.04764\n",
      "Batch number:  8700 lr:  0.00013742975648339164 Test Loss:  3.04399\n",
      "Batch number:  8800 lr:  0.00013560428571889846 Test Loss:  3.17596\n",
      "Batch number:  8900 lr:  0.0001338678442141468 Test Loss:  3.1221\n",
      "Batch number:  9000 lr:  0.0001322160899609027 Test Loss:  3.1951\n",
      "Batch number:  9100 lr:  0.00013064489271317272 Test Loss:  3.14154\n",
      "Batch number:  9200 lr:  0.0001291503236594374 Test Loss:  3.15152\n",
      "Batch number:  9300 lr:  0.00012772864559857617 Test Loss:  3.18035\n",
      "Batch number:  9400 lr:  0.00012637630359491788 Test Loss:  3.23193\n",
      "Batch number:  9500 lr:  0.00012508991608904985 Test Loss:  3.18101\n",
      "Batch number:  9600 lr:  0.0001238662664421581 Test Loss:  3.22556\n",
      "Batch number:  9700 lr:  0.00012270229489275475 Test Loss:  3.2403\n",
      "Batch number:  9800 lr:  0.00012159509090568058 Test Loss:  3.18416\n",
      "Batch number:  9900 lr:  0.00012054188589425116 Test Loss:  3.20494\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # actually initialize our variables\n",
    "    sess.run(init)\n",
    "    \n",
    "    # batch-minimization loop\n",
    "    for i in range(numberOfBatches):\n",
    "        # get this batches data\n",
    "        batch_X, batch_Y = mnist.train.next_batch(batchSize)\n",
    "        \n",
    "        # calculate new learning rate for this batch:\n",
    "        learning_Rate = minLearningRate + (maxLearningRate - minLearningRate) * math.exp(-i/decaySpeed)\n",
    "    \n",
    "        # setup this batches input dictionary\n",
    "        #     - including the new learning rate\n",
    "        #     - and the dropout/probKeep value\n",
    "        train_data = {X_img: batch_X, Y_True: batch_Y, learningRate: learning_Rate, probKeep: PROBKEEP}\n",
    "        \n",
    "        # run the training step on this batch\n",
    "        sess.run(trainStep, feed_dict=train_data)\n",
    "        \n",
    "        # check our accuracy on training and test data \n",
    "        # while resetting dropout! \n",
    "        if i%100 == 0:\n",
    "            # compute our success on the training data\n",
    "            train_data_to_test_on = {X_img: batch_X, Y_True: batch_Y, learningRate: learning_Rate, probKeep: 1.0}\n",
    "            trainAcc, trainLoss = sess.run([accuracy, cross_entropy], feed_dict=train_data_to_test_on)\n",
    "    \n",
    "            # compute our success on the test data\n",
    "            test_data = {X_img: mnist.test.images, Y_True: mnist.test.labels, probKeep: 1.0}\n",
    "            testAcc,testLoss = sess.run([accuracy, cross_entropy], feed_dict=test_data)\n",
    "            # print(\"Train \" + str(i) + \": accuracy:\" + str(trainAcc) + \" loss: \" + str(trainLoss))\n",
    "            # print(\"Test \" + str(i) + \": accuracy:\" + str(testAcc) + \" loss: \" + str(testLoss))\n",
    "            \n",
    "            trainingAccuracyList.append(trainAcc)\n",
    "            trainingLossList.append(trainLoss)\n",
    "            testAccuracyList.append(testAcc)\n",
    "            testLossList.append(testLoss)\n",
    "            \n",
    "            print(\"Batch number: \",i, \"lr: \", learning_Rate, \"Test Loss: \", testLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.subplot(1,2,1);\n",
    "plt.plot(trainingAccuracyList, label=\"Train Acc\");\n",
    "plt.plot(testAccuracyList, label=\"Test Acc\");\n",
    "plt.title(\"Accuracy\");\n",
    "plt.legend();\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1,2,2);\n",
    "plt.plot(trainingLossList, label=\"Test Loss\");\n",
    "plt.plot(testLossList, label=\"Test Loss\");\n",
    "plt.title(\"Loss\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tailLength = -90\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.subplot(1,2,1);\n",
    "plt.plot(trainingAccuracyList[tailLength:], label=\"Train Acc\");\n",
    "plt.plot(testAccuracyList[tailLength:], label=\"Test Acc\");\n",
    "plt.title(\"Accuracy\");\n",
    "plt.legend();\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1,2,2);\n",
    "plt.plot(trainingLossList[tailLength:], label=\"Test Loss\");\n",
    "plt.plot(testLossList[tailLength:], label=\"Test Loss\");\n",
    "plt.title(\"Loss\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resultsDic = {\"trainAcc\": trainingAccuracyList, \"trainLoss\": trainingLossList, \"testAcc\": testAccuracyList, \"testLoss\": testLossList}\n",
    "\n",
    "with open(\"results/mnist-3.1-results.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(resultsDic, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with Previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"results/mnist-3.0-results.txt\", \"rb\") as rp:\n",
    "    prev_resultsDic = pickle.load(rp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tailLength = -90\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.subplot(1,2,1);\n",
    "plt.plot(trainingAccuracyList[tailLength:], label=\"Train Acc\");\n",
    "plt.plot(testAccuracyList[tailLength:], label=\"Test Acc\");\n",
    "\n",
    "plt.plot(prev_resultsDic[\"trainAcc\"][tailLength:], label=\"2.2 - Train Acc\", alpha=0.3, linestyle=':');\n",
    "plt.plot(prev_resultsDic[\"testAcc\"][tailLength:], label=\"2.2 - Test Acc\", alpha=0.3, linestyle=':');\n",
    "\n",
    "plt.title(\"Comparing Amnisccuracy\");\n",
    "plt.legend();\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1,2,2);\n",
    "plt.plot(trainingLossList[tailLength:], label=\"Test Loss\");\n",
    "plt.plot(testLossList[tailLength:], label=\"Test Loss\", linewidth=5);\n",
    "\n",
    "plt.plot(prev_resultsDic[\"trainLoss\"][tailLength:], label=\"2.2 - Train Loss\", alpha=0.3, linestyle=':');\n",
    "plt.plot(prev_resultsDic[\"testLoss\"][tailLength:], label=\"2.2 - Test Loss\", alpha=0.3, linestyle=':');\n",
    "\n",
    "plt.title(\"Loss\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are now almost at 99% accuracy but not quite! Notice we once again have the rise in our test error! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
